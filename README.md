# MLLM-QA-Papers-with-Code

A collection of papers and code repositories for employing Multi-Modal Large Language Models (MLLM) in quality assessment tasks.

| Title | Pub | Paper Link | Code Link |
|-------|------------|------------|-----------|
| Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision | ICLR2024 | [Paper](https://arxiv.org/pdf/2309.14181.pdf) | [Code](https://github.com/Q-Future/Q-Bench) |
| Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models | CVPR2024 | [Paper](https://arxiv.org/pdf/2309.14181.pdf) | [Code](https://arxiv.org/pdf/2311.06783.pdf) |
| Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels | - | [Paper](https://arxiv.org/pdf/2312.17090.pdf) | [Code](https://github.com/Q-Future/Q-Align) |
| Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models | - | [Paper](https://arxiv.org/pdf/2312.08962.pdf) | [Code](https://depictqa.github.io/) |
| Towards Open-ended Visual Quality Comparison | - | [Paper](https://arxiv.org/abs/2402.16641) | [Code](https://github.com/Q-Future/Co-Instruct) |
| VisualCritic: Making LMMs Perceive Visual Quality Like Humans | - | [Paper](https://arxiv.org/pdf/2403.12806v1.pdf) | [Code](#) |
| AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception | - | [Paper](https://arxiv.org/pdf/2401.08276.pdf) | [Code](https://github.com/yipoh/AesBench) |
| 2AFC Prompting of Large Multimodal Models for Image Quality Assessment | - | [Paper](https://arxiv.org/abs/2402.01162) | [Code](https://github.com/h4nwei/2AFC-LMMs) |
| A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment | - | [Paper](https://arxiv.org/abs/2403.10854) | [Code]() |

