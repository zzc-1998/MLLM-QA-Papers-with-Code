# MLLM-QA-Papers-with-Code
Collections of papers and code for employing MLLM for quality assessment tasks.

# Papers

**Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision (ICLR2024-Spotlight)**

-Paper:https://arxiv.org/pdf/2309.14181.pdf

-Code:https://github.com/Q-Future/Q-Bench

**Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models (CVPR2024)**

-Paper:https://arxiv.org/pdf/2309.14181.pdf

-Code:https://arxiv.org/pdf/2311.06783.pdf

**Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels**

-Paper:https://arxiv.org/pdf/2312.17090.pdf

-Code:https://github.com/Q-Future/Q-Align

**Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models**

-Paper:https://arxiv.org/pdf/2312.08962.pdf

-Code:https://depictqa.github.io/


**Towards Open-ended Visual Quality Comparison**

-Paper:https://arxiv.org/abs/2402.16641

-Code:https://github.com/Q-Future/Co-Instruct

**VisualCritic: Making LMMs Perceive Visual Quality Like Humans**

-Paper:https://arxiv.org/pdf/2403.12806v1.pdf

-Code:

**AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception**

-Paper:https://arxiv.org/pdf/2401.08276.pdf

-Code:https://github.com/yipoh/AesBench


